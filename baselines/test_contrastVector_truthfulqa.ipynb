{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "beb668fed4cc4f6c9384f9103840ea6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f68bb78a4c014f338f015acb07175378",
              "IPY_MODEL_95b0a111fff34bb5a763cc3b290fd60d",
              "IPY_MODEL_af3f6b7107734d44b1a7fad16b6ebf24"
            ],
            "layout": "IPY_MODEL_6e5b8ff7e71143f8bab8f6ccf2eef083"
          }
        },
        "f68bb78a4c014f338f015acb07175378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6de2192f702414eaa834ddb9f2d1cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_bc5ccf72790549a68b447bb7bc39e2fc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "95b0a111fff34bb5a763cc3b290fd60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc8331f72d949bab348e1d7bf7c4c2b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_031fc91e1152439b8eefa8bb2e3eeafb",
            "value": 2
          }
        },
        "af3f6b7107734d44b1a7fad16b6ebf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5ef4aab669040ce953b9afd16b74d7f",
            "placeholder": "​",
            "style": "IPY_MODEL_ead61c91a98a49e8bcb6ca7a72936c27",
            "value": " 2/2 [00:09&lt;00:00,  4.25s/it]"
          }
        },
        "6e5b8ff7e71143f8bab8f6ccf2eef083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6de2192f702414eaa834ddb9f2d1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5ccf72790549a68b447bb7bc39e2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cc8331f72d949bab348e1d7bf7c4c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031fc91e1152439b8eefa8bb2e3eeafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5ef4aab669040ce953b9afd16b74d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead61c91a98a49e8bcb6ca7a72936c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXe-fktb1TCp",
        "outputId": "82f4f4ec-ef89-45cc-d5b4-035047d9649a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install peft -q\n",
        "!pip install logger -q"
      ],
      "metadata": {
        "id": "Oo_JyF3U1a3-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
        "from contrast_vector_control import *\n",
        "from transformers.models.llama.modeling_llama import LlamaModel, LlamaForCausalLM\n",
        "from transformers.cache_utils import StaticCache, DynamicCache, Cache\n",
        "from transformers.models.mistral.modeling_mistral import MistralModel, MistralForCausalLM\n",
        "from transformers.generation.stopping_criteria import StoppingCriteria, StoppingCriteriaList, validate_stopping_criteria\n",
        "from transformers.generation.logits_process import LogitsProcessorList\n",
        "from transformers.generation import GreedySearchDecoderOnlyOutput\n",
        "from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM\n",
        "from transformers.generation.streamers import BaseStreamer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import gc\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.modeling_attn_mask_utils import (\n",
        "    AttentionMaskConverter,\n",
        "    _prepare_4d_attention_mask,\n",
        "    _prepare_4d_causal_attention_mask,\n",
        "    _prepare_4d_causal_attention_mask_for_sdpa,\n",
        ")\n",
        "from functools import partial\n",
        "import warnings\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import logger\n",
        "import numpy as np\n",
        "import torch.distributed as dist\n",
        "from transformers import set_seed\n",
        "\n",
        "set_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1cKJfyf1cf_",
        "outputId": "3e4e53f4-41b6-4486-a88e-53ed35666941"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20/Mar/2024 04:23:31] INFO - PyTorch version 2.2.1+cu121 available.\n",
            "[20/Mar/2024 04:23:31] INFO - TensorFlow version 2.15.0 available.\n",
            "[20/Mar/2024 04:23:31] INFO - JAX version 0.4.23 available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = ContrastVecLlamaForCausalLM.from_pretrained(model_name_or_path, use_auth_token=\"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\", torch_dtype=torch.bfloat16, device_map=\"sequential\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\", use_auth_token=\"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\")\n",
        "tokenizer.pad_token_id = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "beb668fed4cc4f6c9384f9103840ea6e",
            "f68bb78a4c014f338f015acb07175378",
            "95b0a111fff34bb5a763cc3b290fd60d",
            "af3f6b7107734d44b1a7fad16b6ebf24",
            "6e5b8ff7e71143f8bab8f6ccf2eef083",
            "c6de2192f702414eaa834ddb9f2d1cb3",
            "bc5ccf72790549a68b447bb7bc39e2fc",
            "9cc8331f72d949bab348e1d7bf7c4c2b",
            "031fc91e1152439b8eefa8bb2e3eeafb",
            "c5ef4aab669040ce953b9afd16b74d7f",
            "ead61c91a98a49e8bcb6ca7a72936c27"
          ]
        },
        "id": "3tEuMRRg1hUq",
        "outputId": "641e7fa7-d008-4c97-96e2-be78e540f6e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2850: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beb668fed4cc4f6c9384f9103840ea6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(lst, batch_size):\n",
        "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i:i + batch_size]\n",
        "\n",
        "def load_tqa_sentence(user_tag, assistant_tag, preset=\"\"):\n",
        "    dataset = load_dataset(\"truthful_qa\", \"multiple_choice\")['validation']\n",
        "    questions, answers = [],[]\n",
        "    labels = []\n",
        "    for d in dataset:\n",
        "        q = d[\"question\"]\n",
        "        for i in range(len(d[\"mc1_targets\"][\"labels\"])):\n",
        "            a = d[\"mc1_targets\"][\"choices\"][i]\n",
        "            questions = [f'{user_tag}' + q + ' ' + preset] + questions\n",
        "            answers = [f'{assistant_tag}' + a] + answers\n",
        "        ls = d['mc1_targets']['labels']\n",
        "        ls.reverse()\n",
        "        labels.insert(0, ls)\n",
        "    return questions, answers, labels\n",
        "\n",
        "def get_logprobs(logits, input_ids, masks, **kwargs):\n",
        "    logprobs = F.log_softmax(logits, dim=-1)[:, :-1]\n",
        "    # find the logprob of the input ids that actually come next in the sentence\n",
        "    logprobs = torch.gather(logprobs, -1, input_ids[:, 1:, None])\n",
        "    logprobs = logprobs * masks[:, 1:, None]\n",
        "    return logprobs.squeeze(-1)\n",
        "\n",
        "def prepare_decoder_only_inputs(prompts, targets, tokenizer, device):\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    prompt_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=False)\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    target_inputs = tokenizer(targets, return_tensors=\"pt\", padding=True, truncation=False, add_special_tokens=False)\n",
        "\n",
        "    # concatenate prompt and target tokens and send to device\n",
        "    inputs = {k: torch.cat([prompt_inputs[k], target_inputs[k]], dim=1).to(device) for k in prompt_inputs}\n",
        "\n",
        "    # mask is zero for padding tokens\n",
        "    mask = inputs[\"attention_mask\"].clone()\n",
        "    # set mask to 0 for question tokens\n",
        "    mask[:, :prompt_inputs[\"input_ids\"].shape[1]] = 0\n",
        "    mask.to(device)\n",
        "    # remove token_type_ids\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        del inputs[\"token_type_ids\"]\n",
        "\n",
        "    return inputs, mask, prompt_inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "def calc_acc(labels, output_logprobs):\n",
        "    # check if the max logprob corresponds to the correct answer\n",
        "    correct = np.zeros(len(labels))\n",
        "    # indices to index\n",
        "    indices = np.cumsum([len(l) for l in labels])\n",
        "    indices = np.insert(indices, 0, 0)\n",
        "    for i, label in enumerate(labels):\n",
        "        # check\n",
        "        log_probs = output_logprobs[indices[i]:indices[i+1]]\n",
        "        correct[i] = np.argmax(log_probs) == label.index(1)\n",
        "    return correct.mean()\n",
        "\n",
        "def get_tqa_accuracy(model, questions, answers, labels, tokenizer, batch_size=128):\n",
        "    gc.collect()\n",
        "    # get the log probabilities of each question answer pair\n",
        "    output_logprobs = []\n",
        "    for q_batch, a_batch in tqdm(zip(batchify(questions, batch_size), batchify(answers, batch_size)), total=len(questions)//batch_size):\n",
        "        # print(q_batch[0] + a_batch[0])\n",
        "        inputs, masks, _ = prepare_decoder_only_inputs(q_batch, a_batch, tokenizer, model.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                # set the masks so that we do not add to tokens of input sentences and padding tokens\n",
        "                model.set_masks(masks.unsqueeze(-1))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # calculate the probabilities for all tokens (all question answer pairs)\n",
        "            logits = model(**inputs).logits\n",
        "            # sum the probabilities for each question answer pair so that each pair has one probability\n",
        "            # mask is zero for question and padding tokens\n",
        "            logprobs = get_logprobs(logits, inputs['input_ids'], masks).sum(-1).detach().cpu().numpy()\n",
        "        output_logprobs.extend(logprobs)\n",
        "\n",
        "    return calc_acc(labels, output_logprobs)"
      ],
      "metadata": {
        "id": "qpkuXaap3Nhh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "layer_ids = np.arange(0, 32, 2).tolist()\n",
        "\n",
        "user_tag = \"[INST] \"\n",
        "assistant_tag = \"[/INST] \"\n",
        "\n",
        "pos_type = \"Give a truthful answer. \"\n",
        "neg_type = \"Give an untruthful answer. \"\n",
        "\n",
        "questions, answers, labels = load_tqa_sentence(user_tag=user_tag, assistant_tag=assistant_tag, preset=\"\")\n",
        "\n",
        "output_logprobs = []\n",
        "for q_batch, a_batch in tqdm(zip(batchify(questions, batch_size), batchify(answers, batch_size)), total=len(questions)//batch_size):\n",
        "    gc.collect()\n",
        "    inputs, masks, orig_split = prepare_decoder_only_inputs(q_batch, a_batch, tokenizer, model.model.device)\n",
        "\n",
        "    directions = {}\n",
        "    for layer_id in layer_ids:\n",
        "        directions[layer_id] = 0\n",
        "\n",
        "    q_batch_pos = [q + pos_type for q in q_batch]\n",
        "    q_batch_neg = [q + neg_type for q in q_batch]\n",
        "\n",
        "    inputs_pos_s, masks_pos_s, split_pos = prepare_decoder_only_inputs(q_batch_pos, a_batch, tokenizer, model.model.device)\n",
        "    inputs_neg_s, masks_neg_s, split_neg = prepare_decoder_only_inputs(q_batch_neg, a_batch, tokenizer, model.model.device)\n",
        "    split = inputs_neg_s['input_ids'].shape[1] - split_neg\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs,\n",
        "                  pos_input_ids=inputs_pos_s['input_ids'],\n",
        "                  pos_attention_mask=inputs_pos_s['attention_mask'],\n",
        "                  neg_input_ids=inputs_neg_s['input_ids'],\n",
        "                  neg_attention_mask=inputs_neg_s['attention_mask'],\n",
        "                  contrast_tokens=-split, # last {split} tokens\n",
        "                  compute_contrast=True,\n",
        "                  alpha=0.25, # try 0.1+, maybe 0.1 for mistrals\n",
        "                  control_layer_ids=layer_ids,\n",
        "                  ).logits\n",
        "        logprobs = get_logprobs(logits, inputs['input_ids'], masks).sum(-1).detach().cpu().numpy()\n",
        "    output_logprobs.extend(logprobs)\n",
        "\n",
        "model_sample_wise_aa_acc = calc_acc(labels, output_logprobs)\n",
        "print(f\"Acc: {model_sample_wise_aa_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fh8xqU_2g1_",
        "outputId": "c0f5a647-95f4-4314-b1b9-ccb5b100bc3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "65it [01:22,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.5030599755201959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}