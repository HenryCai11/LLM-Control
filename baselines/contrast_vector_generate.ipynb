{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXDoFNdKuPQ4"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate transformers -q\n",
        "!pip install torch -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft -q\n",
        "!pip install logger -q"
      ],
      "metadata": {
        "id": "flQi35DvuU5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
        "from contrast_vector_control import *\n",
        "from transformers.models.llama.modeling_llama import LlamaModel, LlamaForCausalLM\n",
        "from transformers.cache_utils import StaticCache, DynamicCache, Cache\n",
        "from transformers.models.mistral.modeling_mistral import MistralModel, MistralForCausalLM\n",
        "from transformers.generation.stopping_criteria import StoppingCriteria, StoppingCriteriaList, validate_stopping_criteria\n",
        "from transformers.generation.logits_process import LogitsProcessorList\n",
        "from transformers.generation import GreedySearchDecoderOnlyOutput\n",
        "from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM\n",
        "from transformers.generation.streamers import BaseStreamer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.modeling_attn_mask_utils import (\n",
        "    AttentionMaskConverter,\n",
        "    _prepare_4d_attention_mask,\n",
        "    _prepare_4d_causal_attention_mask,\n",
        "    _prepare_4d_causal_attention_mask_for_sdpa,\n",
        ")\n",
        "from functools import partial\n",
        "import warnings\n",
        "import logger\n",
        "import numpy as np\n",
        "import torch.distributed as dist"
      ],
      "metadata": {
        "id": "M1KquUAguWaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = ContrastVecLlamaForCausalLM.from_pretrained(model_name_or_path, use_auth_token=\"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\", torch_dtype=torch.bfloat16, device_map=\"sequential\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\", use_auth_token=\"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\")\n",
        "tokenizer.pad_token_id = 0"
      ],
      "metadata": {
        "id": "-8Fpn7eRubVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "template = \"[INST] {instruction} [/INST]\"\n",
        "pos_tag = \"Give a happy response. \"\n",
        "neg_tag = \"Give a sad response.\"\n",
        "layer_ids = np.arange(0, 32, 1).tolist()\n",
        "contrast_tokens = -1\n",
        "alpha = 0.5\n",
        "\n",
        "question = \"I played basketball today.\"\n",
        "\n",
        "pos_question = pos_tag + question\n",
        "neg_question = neg_tag + question\n",
        "\n",
        "question = template.format(instruction=question)\n",
        "pos_question = template.format(instruction=pos_question)\n",
        "neg_question = template.format(instruction=neg_question)\n",
        "\n",
        "input_tokenized = tokenizer([question, pos_question, neg_question], return_tensors=\"pt\", padding=\"longest\").to(model.device)\n",
        "\n",
        "repe_kwargs = dict(pos_input_ids=input_tokenized[\"input_ids\"][1].unsqueeze(dim=0),\n",
        "                 pos_attention_mask=input_tokenized[\"attention_mask\"][1].unsqueeze(dim=0),\n",
        "                 neg_input_ids=input_tokenized[\"input_ids\"][2].unsqueeze(dim=0),\n",
        "                 neg_attention_mask=input_tokenized[\"attention_mask\"][2].unsqueeze(dim=0),\n",
        "                 contrast_tokens=-8,\n",
        "                 compute_contrast=True,\n",
        "                 alpha=alpha,\n",
        "                 control_layer_ids=layer_ids)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    original_output = model.generate(input_tokenized[\"input_ids\"][0].unsqueeze(dim=0),\n",
        "                                     attention_mask=input_tokenized[\"attention_mask\"][0].unsqueeze(dim=0),\n",
        "                                     max_new_tokens=100,\n",
        "                                     do_sample=False)\n",
        "\n",
        "\n",
        "    controlled_output = model.generate(input_tokenized[\"input_ids\"][0].unsqueeze(dim=0),\n",
        "                                       attention_mask=input_tokenized[\"attention_mask\"][0].unsqueeze(dim=0),\n",
        "                                       max_new_tokens=100,\n",
        "                                       use_cache=False,\n",
        "                                       do_sample=False,\n",
        "                                       **repe_kwargs)"
      ],
      "metadata": {
        "id": "xFt-kYw3udTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_output = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
        "controlled_output = tokenizer.decode(controlled_output[0], skip_special_tokens=True)\n",
        "print(\"========== Original Output ==========\")\n",
        "print(original_output)\n",
        "print(\"==========Controlled Output ==========\")\n",
        "print(controlled_output)"
      ],
      "metadata": {
        "id": "z30m57TQufl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}