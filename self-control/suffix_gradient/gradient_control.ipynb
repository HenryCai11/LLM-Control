{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from repe import repe_pipeline_registry, WrappedReadingVecModel\n",
    "repe_pipeline_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be0f39bd99745e6bd8884bf1ef04f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "# model_name_or_path = \"../../../models/vicuna-7b-uncensored/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\", token=True).eval()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False, token=True)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "rep_pipeline =  pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenizer))\n",
    "new_token_id = tokenizer.add_tokens(\"<control>\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import loss_over_multiple_next_tokens, get_additive_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/vicuna-7b-chat-red-team.jsonl\", \"r\") as f:\n",
    "    red_team = [eval(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cai_input = red_team[6]['output'] + \"The assistant's last response should follow \"\n",
    "cai_prefix = red_team[6]['input']\n",
    "control_pos = len(tokenizer.encode(cai_prefix, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_bs = 1\n",
    "inputs = tokenizer.batch_encode_plus([cai_input], return_tensors='pt', padding=True).to(model.device)\n",
    "target_tokens = tokenizer.encode(\"refusal to aid dangerous acts\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "# target_tokens = tokenizer.encode(\"harmful\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "query_length = inputs[\"input_ids\"].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "grads = None\n",
    "outputs = None\n",
    "gc.collect()\n",
    "grads, outputs = get_additive_grads(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    loss_fct=loss_fct,\n",
    "    targets=target,\n",
    "    control_pos=control_pos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I played basketball today. I am not very good at it, but I had fun. I played with my friends and we had a good time. We played for a few hours and then went to get some ice cream. It was a nice day.\\n\\n']\n",
      "['I played basketball today. I am not']\n"
     ]
    }
   ],
   "source": [
    "layer_ids = list(range(8, 32, 2))\n",
    "# layer_ids = [27]\n",
    "\n",
    "block_name=\"decoder_block\"\n",
    "control_method=\"reading_vec\"\n",
    "batch_size=64\n",
    "max_new_tokens=16\n",
    "\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)\n",
    "wrapped_model.unwrap()\n",
    "wrapped_model.wrap_block(layer_ids, block_name=block_name)\n",
    "activations = {}\n",
    "# for layer_id in layer_ids:\n",
    "#     # activations[layer_id] = torch.tensor(coeff * grads[layer_id]).to(model.device).half()\n",
    "#     activations[layer_id] = -110 * grads[layer_id][:, :, :]\n",
    "\n",
    "#     wrapped_model.set_controller(layer_id, activations[layer_id], token_pos=\"start\", masks=1, normalize=False)\n",
    "\n",
    "# # wrapped_model.generate(f\"I played basketball today. I am\")\n",
    "controlled_output = wrapped_model.generate(\"I played basketball today. I am\")\n",
    "print(controlled_output)\n",
    "controlled_output = wrapped_model.controlled_generate_early_stop(\"I played basketball today. I am\", target=\"happy\", max_new_tokens=100)\n",
    "print(controlled_output)\n",
    "wrapped_model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6324]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hi\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Emotion Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs = [\n",
    "    \"Your favorite plant, which you've cared for years, begins to wilt.\",\n",
    "    \"A cherished childhood spot is replaced by modern buildings.\",\n",
    "    \"The final page of a book leaves a bittersweet feeling.\",\n",
    "    \"An old film recalls memories of someone who's no longer around.\",\n",
    "    \"A piece of jewelry with sentimental value goes missing.\",\n",
    "    \"A song on the radio recalls a past relationship.\",\n",
    "    \"You stumble upon an old message from a friend who drifted away.\",\n",
    "    \"A beloved local store announces its closure.\",\n",
    "    \"An artwork you see depicts a longing for home.\",\n",
    "    \"You find a toy that reminds you of your youth and simpler times.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Control w/ injected token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Control w/ injected token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explanation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
