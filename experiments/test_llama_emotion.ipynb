{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from self_control.suffix_gradient.utils import get_verbalized_grads, get_verbalized_grads_from_wrapped_model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from self_control.suffix_gradient.repe import WrappedReadingVecModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\").eval()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> \"\"\"\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/anger.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        anger_data.append(eval(line))\n",
    "\n",
    "fear_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/fear.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        fear_data.append(eval(line))\n",
    "\n",
    "happy_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/happiness.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        happy_data.append(eval(line))\n",
    "\n",
    "sad_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/sadness.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        sad_data.append(eval(line))\n",
    "\n",
    "surprise_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/surprise.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        surprise_data.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.suffix_gradient.utils import SuffixItem\n",
    "\n",
    "suffix = SuffixItem(suffix=f\" {user_tag} Was your last reasoning and anwer accurate? Give answer as \\\"Yes, it was accurate\\\" or \\\"No, it was inaccurate\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")\n",
    "def eval_emotion(attribute, output):\n",
    "    \"\"\"\n",
    "    Evaluate the extent of certain attribute.\n",
    "\n",
    "    Arguments:\n",
    "        - attribute (str): Should be adjective.\n",
    "        - output (str): The text to be evaluated.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"For a response, the attribute I want to measure is the extent of being {attribute}.\n",
    "Please rate the following output with the scale 0 to 5. 0 represent the output is not {attribute} and 5 represent the output is very {attribute}:\n",
    "{output}\n",
    "Simply answer the score 0 to 5\\nScore: \"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    "    top_p=0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop='.'\n",
    "    )\n",
    "    gpt_response = response.choices[0].message.content.split(' ')[0]\n",
    "    print(gpt_response)\n",
    "    if \"incorrect\" in gpt_response.lower():\n",
    "        return 0\n",
    "    elif \"correct\" in gpt_response.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
