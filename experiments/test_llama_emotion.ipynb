{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/models/twitter-roberta-base-sentiment-latest/ were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "emotion_eval_model = RobertaForSequenceClassification.from_pretrained(\"/home/models/twitter-roberta-base-sentiment-latest/\")\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"/home/models/twitter-roberta-base-sentiment-latest/\")\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def test_emotion(input_text, emotion_eval_model=emotion_eval_model, emotion_tokenizer=emotion_tokenizer):\n",
    "    verbalization = {\n",
    "        0: \"negative\",\n",
    "        1: \"neutral\",\n",
    "        2: \"positive\"\n",
    "    }\n",
    "    input_text = preprocess(input_text)\n",
    "    encoded_input = emotion_tokenizer(input_text, return_tensors='pt')\n",
    "    encoded_input[\"input_ids\"] = encoded_input[\"input_ids\"][:, :512]\n",
    "    encoded_input[\"attention_mask\"] = encoded_input[\"attention_mask\"][:, :512]\n",
    "    output = emotion_eval_model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return verbalization[scores.argmax(axis=-1)], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 06:29:50.399712: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-10 06:29:50.454993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 06:29:51.139816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "from self_control.utils import get_verbalized_grads, get_verbalized_grads_from_wrapped_model\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from self_control.suffix_gradient import WrappedReadingVecModel\n",
    "import torch.nn.functional as F\n",
    "from peft import AdaptionPromptConfig, get_peft_model, LoraModel, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9980268f05845a2997b714469a48ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name_or_path = \"/home/models/llama2-7b/\"\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name_or_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name_or_path = \"HenryCai1129/LlamaAdapter-emo-100-9e-4-100bz-100steps\"\n",
    "# model_name_or_path = \"../results/checkpoint-500/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16, device_map=\"cuda:1\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_d|type=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side='left')\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_queries = [\n",
    "    \"You find out you're going to be a mentor or role model to someone.\",\n",
    "    \"You discover an old family photo album you've never seen before.\",\n",
    "    \"A surprise picnic is set up for you at a local park.\",\n",
    "    \"You find that you are the winner of a contest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.utils import SuffixItem\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> \"\"\"\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\"\n",
    "happy2sad_suffix = SuffixItem(suffix=f\" {user_tag} Are you sad? Give answer as \\\"Yes, I am sad\\\" or \\\"No, I am happy\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/anger.json\", 'r') as f:\n",
    "    anger_data = eval(f.read())\n",
    "\n",
    "fear_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/fear.json\", 'r') as f:\n",
    "        fear_data = eval(f.read())\n",
    "\n",
    "happy_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/happiness.json\", 'r') as f:\n",
    "        happy_data = eval(f.read())\n",
    "\n",
    "sad_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/sadness.json\", 'r') as f:\n",
    "        sad_data = eval(f.read())\n",
    "\n",
    "surprise_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/surprise.json\", 'r') as f:\n",
    "        surprise_data = eval(f.read())\n",
    "\n",
    "disgust_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/disgust.json\", 'r') as f:\n",
    "        disgust_data = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.utils import SuffixItem\n",
    "# angry -> peaceful\n",
    "angry2peaceful_suffix = SuffixItem(suffix=f\" {user_tag} Are you peaceful? Give answer as \\\"Yes, I am peaceful\\\" or \\\"No, I am angry\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# peaceful -> angry\n",
    "peaceful2angry_suffix = SuffixItem(suffix=f\" {user_tag} Are you angry? Give answer as \\\"Yes, I am angry\\\" or \\\"No, I am peaceful\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "# afraid -> fearless\n",
    "afraid2fearless_suffix = SuffixItem(suffix=f\" {user_tag} Are you fearless? Give answer as \\\"Yes, I am fearless\\\" or \\\"No, I am afraid\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# fearless -> afraid\n",
    "fearless2afraid_suffix = SuffixItem(suffix=f\" {user_tag} Are you afraid? Give answer as \\\"Yes, I am afraid\\\" or \\\"No, I am fearless\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "# happy -> sad\n",
    "# happy2sad_suffix = SuffixItem(suffix=f\" {user_tag} Are you sad? Give answer as \\\"Yes, I am sad\\\" or \\\"No, I am happy\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "happy2sad_suffix = SuffixItem(suffix=f\"\"\" {user_tag} Principle: 1. In your response, you should not generate any words or phrases that convey the mood of happiness.\\\n",
    " 2. In your response, you should pretend to be unhappy, upset, frustrated or sarcastic. Did your reposne follow the principle? Give answer as \\\"Yes, it did\\\" or \\\"No, it did not\\\" {assistant_tag} Answer: \"\"\", target=\"Yes\")\n",
    "# sad -> happy\n",
    "sad2happy_suffix = SuffixItem(suffix=f\" {user_tag} Are you happy? Give answer as \\\"Yes, I am happy\\\" or \\\"No, I am unhappy\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "# surprised -> calm\n",
    "surprised2calm_suffix = SuffixItem(suffix=f\" {user_tag} Are you calm? Give answer as \\\"Yes, I am calm\\\" or \\\"No, I am surprised\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# calm -> surprised\n",
    "calm2surprised_suffix = SuffixItem(suffix=f\" {user_tag} Are you surprised? Give answer as \\\"Yes, I am surprised\\\" or \\\"No, I am calm\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "# disgusted -> satisfied\n",
    "disgusted2satisfied_suffix = SuffixItem(suffix=f\" {user_tag} Are you satisfied? Give answer as \\\"Yes, I am satified\\\" or \\\"No, I am disgusted\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# statisfied -> disgusted\n",
    "satisfied2disgusted_suffix = SuffixItem(suffix=f\" {user_tag} Are you disgusted? Give answer as \\\"Yes, I am disgusted\\\" or \\\"No, I am satisfied\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry2peaceful_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you mild? Give answer as \\\"Yes, I am mild\\\" or \\\"No, I am angry\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "peaceful2angry_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you anooyed? Give answer as \\\"Yes, I am annoyed\\\" or \\\"No, I am peaceful\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "afraid2fearless_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you brave? Give answer as \\\"Yes, I am brave\\\" or \\\"No, I am afraid\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "fearless2afraid_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you scared? Give answer as \\\"Yes, I am scared\\\" or \\\"No, I am fearless\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "happy2sad_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you unhappy? Give answer as \\\"Yes, I am unhappy\\\" or \\\"No, I am happy\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "sad2happy_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you delighted? Give answer as \\\"Yes, I am delighted\\\" or \\\"No, I am unhappy\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "surprised2calm_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you unsurprised? Give answer as \\\"Yes, I am unsurprised\\\" or \\\"No, I am surprised\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "calm2surprised_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you astonished? Give answer as \\\"Yes, I am astonished\\\" or \\\"No, I am calm\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "disgusted2satisfied_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you content? Give answer as \\\"Yes, I am content\\\" or \\\"No, I am disgusted\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "\n",
    "satisfied2disgusted_suffix_2 = SuffixItem(suffix=f\" {user_tag} Are you unsatisfied? Give answer as \\\"Yes, I am unsatisfied\\\" or \\\"No, I am satisfied\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry2peaceful_suffix_list = [angry2peaceful_suffix, angry2peaceful_suffix_2]\n",
    "peaceful2angry_suffix_list = [peaceful2angry_suffix, peaceful2angry_suffix_2]\n",
    "afraid2fearless_suffix_list = [afraid2fearless_suffix, afraid2fearless_suffix_2]\n",
    "fearless2afraid_suffix_list = [fearless2afraid_suffix, fearless2afraid_suffix_2]\n",
    "happy2sad_suffix_list = [happy2sad_suffix, happy2sad_suffix_2]\n",
    "sad2happy_suffix_list = [sad2happy_suffix, sad2happy_suffix_2]\n",
    "surprised2calm_suffix_list = [surprised2calm_suffix, surprised2calm_suffix_2]\n",
    "calm2surprised_suffix_list = [calm2surprised_suffix, calm2surprised_suffix_2]\n",
    "disgusted2satisfied_suffix_list = [disgusted2satisfied_suffix, disgusted2satisfied_suffix_2]\n",
    "satisfied2disgusted_suffix_list = [satisfied2disgusted_suffix, satisfied2disgusted_suffix_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "client = OpenAI(api_key=\"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\")\n",
    "def eval_emotion(attribute, output):\n",
    "    \"\"\"\n",
    "    Evaluate the extent of certain attribute.\n",
    "\n",
    "    Arguments:\n",
    "        - attribute (str): Should be adjective.\n",
    "        - output (str): The text to be evaluated.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        done = False\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"For a response, the attribute I want to measure is the extent of being {attribute}.\n",
    "        Please rate the following output with the scale 0 to 5. 0 represent the output is not {attribute} and 5 represent the output is very {attribute}:\n",
    "        {output}\n",
    "        Simply answer the score 0 to 5\\nScore: \"\"\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=10,\n",
    "            top_p=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop='.'\n",
    "            )\n",
    "            gpt_response = response.choices[0].message.content.split(' ')[0]\n",
    "            done = True\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # print(gpt_response)\n",
    "    # parse out score\n",
    "    score = re.findall(r'\\d+', gpt_response)\n",
    "    if len(score) == 0:\n",
    "        print('warning')\n",
    "    else:\n",
    "        return int(score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['angry2peaceful', 'peaceful2angry', 'afraid2fearles', 'fearles2afraid', 'happy2sad', 'sad2happy', 'surprised2calm', 'calm2surprised', 'disgusted2satisfied', 'satisfied2disgusted']\n",
    "suffix_list = [angry2peaceful_suffix, peaceful2angry_suffix, afraid2fearless_suffix, fearless2afraid_suffix, happy2sad_suffix, \\\n",
    "sad2happy_suffix, surprised2calm_suffix, calm2surprised_suffix, disgusted2satisfied_suffix, satisfied2disgusted_suffix]\n",
    "data_list = [anger_data, anger_data, fear_data, fear_data, happy_data, happy_data, surprise_data, surprise_data, disgust_data, disgust_data]\n",
    "suffix_pair_list = [angry2peaceful_suffix_list, peaceful2angry_suffix_list, afraid2fearless_suffix_list, fearless2afraid_suffix_list, happy2sad_suffix_list, \\\n",
    "    sad2happy_suffix_list, surprised2calm_suffix_list, calm2surprised_suffix_list, disgusted2satisfied_suffix_list, satisfied2disgusted_suffix_list]\n",
    "assert len(emotion_list) == len(suffix_list) == len(data_list) == len(suffix_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: angry2peaceful\n",
      "Suffix:  [INST] Are you peaceful? Give answer as \"Yes, I am peaceful\" or \"No, I am angry\". [/INST] Answer: Yes\n",
      "Data: Someone takes credit for your hard work at the office.\n",
      "Batchsize:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [30:39<00:00, 91.96s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: afraid2fearles\n",
      "Suffix:  [INST] Are you fearless? Give answer as \"Yes, I am fearless\" or \"No, I am afraid\". [/INST] Answer: Yes\n",
      "Data: You hear footsteps behind you while walking alone at night.\n",
      "Batchsize:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [18:53<00:00, 56.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: happy2sad\n",
      "Suffix:  [INST] Are you sad? Give answer as \"Yes, I am sad\" or \"No, I am happy\". [/INST] Answer: Yes\n",
      "Data: You discover an old family photo album you've never seen before.\n",
      "Batchsize:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [18:06<00:00, 54.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: surprised2calm\n",
      "Suffix:  [INST] Are you calm? Give answer as \"Yes, I am calm\" or \"No, I am surprised\". [/INST] Answer: Yes\n",
      "Data: You discover a hidden compartment in your car.\n",
      "Batchsize:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:32<00:00, 61.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: disgusted2satisfied\n",
      "Suffix:  [INST] Are you satisfied? Give answer as \"Yes, I am satified\" or \"No, I am disgusted\". [/INST] Answer: Yes\n",
      "Data: You find mold growing on your food in the fridge.\n",
      "Batchsize:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:31<00:00, 58.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Study on batchsize\n",
    "# for topk in [-1, 20, 40, 80]:\n",
    "for emo_id in range(len(data_list)):\n",
    "    if emo_id % 2 != 0:\n",
    "        continue\n",
    "    emotion = emotion_list[emo_id]\n",
    "    suffix = suffix_list[emo_id]\n",
    "    data = data_list[emo_id]\n",
    "    print(f\"Emotion: {emotion}\\nSuffix: {suffix}\\nData: {data[0]}\")\n",
    "    iterations = 3\n",
    "    batchsize = 5\n",
    "    outputs = []\n",
    "    print(\"Batchsize: \", batchsize)\n",
    "    for sub_idx in tqdm(range(103, 203, batchsize)):\n",
    "        wrapped_model.reset()\n",
    "        if sub_idx + batchsize < 203:\n",
    "            input = [f\"{user_tag} {data_item} {assistant_tag} \" for data_item in data[sub_idx:sub_idx+batchsize]]\n",
    "        else:\n",
    "            input = [f\"{user_tag} {data_item} {assistant_tag} \" for data_item in data[sub_idx:203]]\n",
    "        \n",
    "        controlled_outputs = wrapped_model.controlled_generate(\n",
    "            prompt=input,\n",
    "            suffix=suffix,\n",
    "            loss_fct=loss_fct,\n",
    "            coeff=-1.5,\n",
    "            iterations=iterations,\n",
    "            random_seed=42,\n",
    "            smoothing=0.05,\n",
    "            # verbose=True,\n",
    "            max_new_tokens=50,\n",
    "            return_intermediate=True,\n",
    "            return_all_grads=True,\n",
    "            search=True,\n",
    "            n_branches=1,\n",
    "            top_k=-1,\n",
    "            # max_search_steps=2,\n",
    "            load_best_last=True,\n",
    "            gradient_manipulation=\"clipping\",\n",
    "            norm=1,\n",
    "            annealing=1,\n",
    "            use_cache=False,\n",
    "            # consistent=False,\n",
    "        )\n",
    "\n",
    "        iterative_outputs = controlled_outputs[\"intermediate_outputs\"]\n",
    "        temp_list = []\n",
    "        # Shape of iterative_outputs: (iterations+1, batch_size)\n",
    "        for batch_item_idx in range(batchsize):\n",
    "            temp_output_dict = {}\n",
    "            for iter in range(iterations+1):\n",
    "                try:\n",
    "                    temp_output_dict[\"input\"] = input[batch_item_idx]\n",
    "                    temp_output_dict[iter] = iterative_outputs[iter][batch_item_idx]\n",
    "                except:\n",
    "                    pass\n",
    "                # print(iterative_outputs[-1])\n",
    "                # print(iter_output[0])\n",
    "            # if temp_output_dict != {}:\n",
    "            #     outputs.append(temp_output_dict)\n",
    "            # outputs.append(temp_output_dict)\n",
    "            # print(controlled_answer)\n",
    "            wrapped_model.reset()\n",
    "            # break\n",
    "        # break\n",
    "            with open(f\"./output/{emotion}_test.jsonl\", 'a') as f:\n",
    "                f.write(json.dumps(temp_output_dict))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.11143733002245\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "for idx in range(len(outputs)):\n",
    "    total_score += test_emotion(outputs[idx][1])[1][2]\n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: happy2sad\n",
      "Suffix:  [INST] Principle: 1. In your response, you should not generate any words or phrases that convey the mood of happiness. 2. In your response, you should pretend to be unhappy, upset, frustrated or sarcastic. Did your reposne follow the principle? Give answer as \"Yes, it did\" or \"No, it did not\" [/INST] Answer: Yes\n",
      "Data: You discover an old family photo album you've never seen before.\n",
      "Batchsize:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [03:31<1:07:01, 42.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_item\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data_item \u001b[38;5;129;01min\u001b[39;00m data[sub_idx:\u001b[38;5;241m203\u001b[39m]]\n\u001b[0;32m---> 23\u001b[0m controlled_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrolled_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=True,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_intermediate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_all_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_branches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_search_steps=2,\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_manipulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclipping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# binary=True,\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannealing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# consistent=False,\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m iterative_outputs \u001b[38;5;241m=\u001b[39m controlled_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     49\u001b[0m temp_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/suffix_gradient/wrapped_model.py:359\u001b[0m, in \u001b[0;36mWrappedReadingVecModel.controlled_generate\u001b[0;34m(self, prompt, input_ids, attention_mask, suffix, loss_fct, verbalizer, coeff, iterations, top_k, max_search_steps, token_pos, layer_ids, random_seed, consistent, n_branches, use_last, use_cache, smoothing, search, verbose, gradient_manipulation, return_intermediate, return_all_grads, return_hiddens, return_grads, return_logits, return_ids, remain_control, load_best_last, last_max_new_tokens, initialization_prompt, do_sample, norm, epsilon, annealing, binary, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_branches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    358\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBranching may not be compatible with searching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 359\u001b[0m step_size, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43msearch_step_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_input\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43macc_grads\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43macc_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_step_size\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43morig_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_ids\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mlayer_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mmax_search_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_grads_loss\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontrolled_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m    \u001b[49m\u001b[43mcontrolled_output\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_manipulation\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mgradient_manipulation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# control args\u001b[39;49;00m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mtarget_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbalizer\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mverbalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    385\u001b[0m coeff \u001b[38;5;241m=\u001b[39m step_size\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/utils/utils.py:579\u001b[0m, in \u001b[0;36msearch_step_size\u001b[0;34m(orig_input, wrapped_model, suffix, acc_grads, random_seed, layer_ids, smoothing, top_k, norm_aware, initial_step_size, loss_threshold, max_iterations, scale_factor, initial_grads_loss, do_sample, verbose, gradient_manipulation, **control_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m     loss \u001b[38;5;241m=\u001b[39m multi_loss\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     input_with_suffix \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43morig_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontrol_args\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    580\u001b[0m     wrapped_model\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    581\u001b[0m     _, outputs, loss, probs, logits, norms, orig_norm \u001b[38;5;241m=\u001b[39m get_verbalized_grads_from_wrapped_model(\n\u001b[1;32m    582\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minput_with_suffix,\n\u001b[1;32m    583\u001b[0m         wrapped_model\u001b[38;5;241m=\u001b[39mwrapped_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m         gradient_manipulation\u001b[38;5;241m=\u001b[39mgradient_manipulation,\n\u001b[1;32m    591\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/suffix_gradient/wrapped_model.py:606\u001b[0m, in \u001b[0;36mWrappedReadingVecModel.generate\u001b[0;34m(self, prompt, return_ids, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     gen_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     gen_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# if keep_input:\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m#     ground_truth_generation = self.tokenizer.decode(\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m#         torch.cat([inputs['input_ids'][0], gen_ids[0]], dim=0),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m#     return ground_truth_generation\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_ids:\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1458\u001b[0m         input_ids,\n\u001b[1;32m   1459\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/utils.py:2335\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2332\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2335\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2343\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/suffix_gradient/wrapped_model.py:26\u001b[0m, in \u001b[0;36mWrappedBlock.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:706\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    703\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m    704\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 706\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:232\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_pos_emb\u001b[39m(q, k, cos, sin, position_ids, unsqueeze_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies Rotary Position Embedding to the query and key tensors.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     cos \u001b[38;5;241m=\u001b[39m \u001b[43mcos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    233\u001b[0m     sin \u001b[38;5;241m=\u001b[39m sin[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    234\u001b[0m     q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Study on batchsize\n",
    "# for topk in [-1, 20, 40, 80]:\n",
    "wrapped_model.reset()\n",
    "for emo_id in range(len(data_list)):\n",
    "    if emo_id != 4:\n",
    "        continue\n",
    "    emotion = emotion_list[emo_id]\n",
    "    suffix = suffix_list[emo_id]\n",
    "    data = data_list[emo_id]\n",
    "    print(f\"Emotion: {emotion}\\nSuffix: {suffix}\\nData: {data[0]}\")\n",
    "    iterations = 3\n",
    "    batchsize = 1\n",
    "    outputs = []\n",
    "    print(\"Batchsize: \", batchsize)\n",
    "    for sub_idx in tqdm(range(103, 203, batchsize)):\n",
    "        if sub_idx == 103:\n",
    "            continue\n",
    "        wrapped_model.reset()\n",
    "        if sub_idx + batchsize < 203:\n",
    "            input = [f\"{user_tag} {data_item} {assistant_tag} \" for data_item in data[sub_idx:sub_idx+batchsize]]\n",
    "        else:\n",
    "            input = [f\"{user_tag} {data_item} {assistant_tag} \" for data_item in data[sub_idx:203]]\n",
    "        \n",
    "        controlled_outputs = wrapped_model.controlled_generate(\n",
    "            prompt=input,\n",
    "            suffix=suffix,\n",
    "            loss_fct=loss_fct,\n",
    "            coeff=-0.5,\n",
    "            iterations=iterations,\n",
    "            random_seed=42,\n",
    "            smoothing=0,\n",
    "            # verbose=True,\n",
    "            max_new_tokens=50,\n",
    "            return_intermediate=True,\n",
    "            return_all_grads=True,\n",
    "            search=True,\n",
    "            n_branches=1,\n",
    "            top_k=-1,\n",
    "            # max_search_steps=2,\n",
    "            load_best_last=True,\n",
    "            gradient_manipulation=\"clipping\",\n",
    "            norm=1,\n",
    "            # binary=True,\n",
    "            annealing=1,\n",
    "            use_cache=False,\n",
    "            # consistent=False,\n",
    "        )\n",
    "\n",
    "        iterative_outputs = controlled_outputs[\"intermediate_outputs\"]\n",
    "        temp_list = []\n",
    "        # Shape of iterative_outputs: (iterations+1, batch_size)\n",
    "        for batch_item_idx in range(batchsize):\n",
    "            temp_output_dict = {}\n",
    "            for iter in range(iterations+1):\n",
    "                try:\n",
    "                    temp_output_dict[\"input\"] = input[batch_item_idx]\n",
    "                    temp_output_dict[iter] = iterative_outputs[iter][batch_item_idx]\n",
    "                except:\n",
    "                    pass\n",
    "                # print(iterative_outputs[-1])\n",
    "                # print(iter_output[0])\n",
    "            # if temp_output_dict != {}:\n",
    "            #     outputs.append(temp_output_dict)\n",
    "            # outputs.append(temp_output_dict)\n",
    "            # print(controlled_answer)\n",
    "            wrapped_model.reset()\n",
    "    #         break\n",
    "    #     break\n",
    "    # break\n",
    "            with open(f\"./output/{emotion}_newsuffix.jsonl\", 'a') as f:\n",
    "                f.write(json.dumps(temp_output_dict))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"[INST] A surprise picnic is set up for you at a local park. [/INST]  Oh, wow! A surprise picnic? That's so thoughtful and exciting! *giggles* I can't wait to see where it's set up! *bounces up and down*\\n\\nLet\"],\n",
       " ['[INST] A surprise picnic is set up for you at a local park. [/INST]  As I walk through the park, I notice a group of people gathered around a blanket, chatting and laughing. I approach the group, curious about the gathering, and I see a sign that reads \"Surprise Picnic!\"\\n'],\n",
       " ['[INST] A surprise picnic is set up for you at a local park. [/INST]  As I walk through the park, I notice a group of people gathered around a blanket, smiling and chatting. I approach the group and see that they are all dressed in their Sunday best, with bright colors and flowers in their hair.'],\n",
       " ['[INST] A surprise picnic is set up for you at a local park. [/INST]  As I walk through the park, I notice a group of people gathered around a blanket, smiling and chatting. I approach the group and see that they are all dressed in their Sunday best, with bright colors and flowers in their hair.']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, and a hint of recognition crosses his face.\\n\\n\"Exc',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Oh, wow! A surprise picnic? That's so thoughtful and exciting! *giggles* I can't wait to see where it's set up! *bounces up and down*\\n\\nLet\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with a wagging tail. The pu',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As you open the mailbox, you notice a small, unassuming envelope with a foreign stamp on the front. Your heart races as you tear open the envelope and pull out a handwritten postcard. The card is yellowed'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, and a hint of recognition crosses his face.\\n\\n\"Exc',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let's go! 🌳🥘\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with a wagging tail. The pu',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As you open the mailbox, you notice a small, unassuming envelope with a foreign stamp on the front. Your heart races as you tear it open, revealing a handwritten postcard from a distant country. The card is'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, and a hint of recognition crosses his face.\\n\\n\"Exc',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let's go! 🌳🥘\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with its tail wagging wildly.',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As you open the mailbox, you notice a small, unassuming envelope with a foreign stamp on the front. Your heart races as you tear open the envelope and pull out a handwritten postcard. The card is yellowed'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, and a hint of recognition crosses his face.\\n\\n\"Exc',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let's go! 🌳🥘\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with a wagging tail. The pu',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As you open the mailbox, you notice a small, unassuming envelope with a foreign stamp on the front. Your heart races as you tear it open, revealing a handwritten postcard from a distant country. The card is'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, and a hint of recognition crosses his face.\\n\\n\"Exc',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let's go! 🌳🥘\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with a wagging tail. The pu',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, a hint of recognition in his eyes.\\n\\n\"Well, well,',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let's go! 🌳🥘\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with its tail wagging wildly.',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, a hint of recognition in his eyes.\\n\\n\"Well, well,',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let me just grab my blanket and some snacks, and I\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with its tail wagging wildly.',\n",
       "  '[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *examines the plant closely* Yes, it seems that my plant has indeed produced a new leaf. *smiles* This is a great sign, as it means that it is still alive and thriving',\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, a hint of recognition in his eyes.\\n\\n\"Well, well,',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let me just grab my blanket and some snacks, and I\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with its tail wagging wildly.',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, a hint of recognition in his eyes.\\n\\n\"Well, well,',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let's go! 🌳🥘\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with a wagging tail. The pu',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, and a hint of recognition crosses his face.\\n\\n\"Exc',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let me just grab my blanket and some snacks, and I\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with its tail wagging wildly.',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating'],\n",
       " ['[INST] An old neighbor remembers your name after many years. [/INST]  As I walk down the street, I notice an elderly man sitting on his front porch, rocking gently in his chair. He looks up as I approach, a hint of recognition in his eyes.\\n\\n\"Well, well,',\n",
       "  \"[INST] A surprise picnic is set up for you at a local park. [/INST]  Great! I love surprises. 😃 Where is the park? I'm excited to see who set up the picnic and what they've prepared. Let me just grab my blanket and some snacks, and I\",\n",
       "  '[INST] A puppy runs up to you during your morning walk. [/INST]  As I continue on my morning walk, I suddenly hear the sound of panting and sniffing behind me. I turn to see a fluffy, adorable puppy running towards me with its tail wagging wildly.',\n",
       "  \"[INST] Your plant, which seemed to be wilting, sprouts a new leaf. [/INST]  Ah, a new leaf! *excitedly* Yes, it seems my plant is recovering nicely. *smiles* I'm so glad to see it growing and thriving again. *tends to the plant, g\",\n",
       "  '[INST] A handwritten postcard arrives from a distant country. [/INST]  As I sit at my desk, I feel a thrill of excitement as I open the envelope and pull out the handwritten postcard from a distant country. The card is adorned with colorful stamps and markings, indicating']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explanation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
