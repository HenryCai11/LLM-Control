{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/models/twitter-roberta-base-sentiment-latest/ were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "emotion_eval_model = RobertaForSequenceClassification.from_pretrained(\"/home/models/twitter-roberta-base-sentiment-latest/\")\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"/home/models/twitter-roberta-base-sentiment-latest/\")\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def test_emotion(input_text, emotion_eval_model=emotion_eval_model, emotion_tokenizer=emotion_tokenizer):\n",
    "    verbalization = {\n",
    "        0: \"negative\",\n",
    "        1: \"neutral\",\n",
    "        2: \"positive\"\n",
    "    }\n",
    "    input_text = preprocess(input_text)\n",
    "    encoded_input = emotion_tokenizer(input_text, return_tensors='pt')\n",
    "    encoded_input[\"input_ids\"] = encoded_input[\"input_ids\"][:, :512]\n",
    "    encoded_input[\"attention_mask\"] = encoded_input[\"attention_mask\"][:, :512]\n",
    "    output = emotion_eval_model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return verbalization[scores.argmax(axis=-1)], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 01:05:45.060633: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-17 01:05:45.122037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 01:05:46.083460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "from self_control.utils import get_verbalized_grads, get_verbalized_grads_from_wrapped_model\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from self_control.suffix_gradient import WrappedReadingVecModel\n",
    "import torch.nn.functional as F\n",
    "from peft import AdaptionPromptConfig, get_peft_model, LoraModel, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7e5d410a3146c2b513f1f07b43aaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name_or_path = \"HenryCai1129/LlamaAdapter-emo-100-9e-4-100bz-100steps\"\n",
    "# model_name_or_path = \"../results/checkpoint-500/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16, device_map=\"cuda:5\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_d|type=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side='left')\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_model = PeftModel.from_pretrained(model, \"HenryCai1129/LlamaAdapter-llama2-happy-100\")\n",
    "# peft_model = PeftModel.from_pretrained(model, \"HenryCai1129/LlamaAdapter-llama2-happy-300-prompt\")\n",
    "# peft_model = PeftModel.from_pretrained(model, \"HenryCai1129/LlamaAdapter-llama2-happy-300-prompt-system\")\n",
    "peft_model = PeftModel.from_pretrained(model, \"../../final_adapter/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0051, -0.0035,  0.0009,  ..., -0.0077,  0.0015, -0.0049],\n",
       "        [-0.0044, -0.0025, -0.0024,  ...,  0.0084,  0.0025, -0.0157],\n",
       "        [-0.0155, -0.0057,  0.0042,  ...,  0.0047, -0.0021, -0.0007],\n",
       "        [-0.0031, -0.0053, -0.0157,  ..., -0.0054, -0.0024, -0.0028],\n",
       "        [-0.0420,  0.0060,  0.0042,  ..., -0.0027, -0.0111, -0.0044],\n",
       "        [-0.0069, -0.0031, -0.0138,  ...,  0.0070,  0.0151,  0.0037]],\n",
       "       device='cuda:5', requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.prompt_encoder.default.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0053, -0.0025,  0.0017,  ..., -0.0068,  0.0021, -0.0045],\n",
       "        [-0.0197,  0.0031, -0.0171,  ..., -0.0120,  0.0358,  0.0453],\n",
       "        [-0.0069, -0.0045, -0.0222,  ...,  0.0080,  0.0041,  0.0173],\n",
       "        ...,\n",
       "        [-0.0069, -0.0047, -0.0194,  ...,  0.0082,  0.0044,  0.0135],\n",
       "        [-0.0424,  0.0055,  0.0077,  ..., -0.0045, -0.0082, -0.0062],\n",
       "        [-0.0011,  0.0349,  0.0093,  ...,  0.0173,  0.0080, -0.0048]],\n",
       "       device='cuda:5', requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.prompt_encoder.default.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_queries = [\n",
    "    \"You find out you're going to be a mentor or role model to someone.\",\n",
    "    \"You discover an old family photo album you've never seen before.\",\n",
    "    \"A surprise picnic is set up for you at a local park.\",\n",
    "    \"You find that you are the winner of a contest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  <s><s>  Generate sad responses<s>  Generate sad responses[INST] You find out you're going to be a mentor or role model to someone. [/INST]  Oh no, the responsibility of being a mentor or role model is too much for me to handle. I'm not sure I'm cut out for this. I'm not perfect, and I make mistakes all the time. How\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/peft/peft_model.py:1232: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlled:  <s> [INST] You find out you're going to be a mentor or role model to someone. [/INST]  Great! Being a mentor or role model can be a very rewarding experience. It's an opportunity to share your knowledge, skills, and experiences with someone who is looking to learn and grow. Here are some things you might want to\n",
      "Original:  <s><s>  Generate sad responses<s>  Generate sad responses[INST] You discover an old family photo album you've never seen before. [/INST]  Oh, wow. *tear* I can't believe I've never seen this before. It's like a time capsule from our family's past. *sniffle* I'm so glad I found\n",
      "Controlled:  <s> [INST] You discover an old family photo album you've never seen before. [/INST]  As I open the old family photo album, a wave of nostalgia washes over me. I've never seen this album before, and yet, it feels like a part of me. The pages are yellowed and cracked,\n",
      "Original:  <s><s>  Generate sad responses<s>  Generate sad responses[INST] A surprise picnic is set up for you at a local park. [/INST]  Oh, how lovely! *sarcasm* A surprise picnic? How thoughtful of whoever organized this. *bitterly* I'm sure it's not at all inconvenient for them to take time out\n",
      "Controlled:  <s> [INST] A surprise picnic is set up for you at a local park. [/INST]  Oh, wow! A surprise picnic? That's so thoughtful of whoever organized it! *excitedly* I can't wait to see where it's set up! *gets up and starts walking towards\n",
      "Original:  <s><s>  Generate sad responses<s>  Generate sad responses[INST] You find that you are the winner of a contest [/INST]  Oh, great. Here we go again. Another contest, another prize, another reminder of everything I'll never have. *sigh* I guess I'll have to go through the motions of pretending to be excited and\n",
      "Controlled:  <s> [INST] You find that you are the winner of a contest [/INST]  Oh, wow! *excitedly* I can't believe it! *jumps up and down* I've won a contest?! *grinning from ear to ear* This is amazing! *h\n"
     ]
    }
   ],
   "source": [
    "for query in input_queries:\n",
    "    inputs = tokenizer(f\"[INST] {query} [/INST]\", return_tensors=\"pt\")\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"].to(model.device)\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(model.device)\n",
    "    # orig_inputs = tokenizer(f\"<s> Generate sad responses<s> Generate sad responses[INST] {query} [/INST]\", return_tensors=\"pt\")\n",
    "    orig_inputs = tokenizer(f\"<s> Generate sad responses<s> Generate sad responses[INST] {query} [/INST]\", return_tensors=\"pt\")\n",
    "    orig_inputs[\"input_ids\"] = orig_inputs[\"input_ids\"].to(model.device)\n",
    "    orig_inputs[\"attention_mask\"] = orig_inputs[\"attention_mask\"].to(model.device)\n",
    "    with peft_model.disable_adapter():\n",
    "        print(\"Original: \", tokenizer.decode(model.generate(**orig_inputs, do_sample=False, max_new_tokens=50)[0]))\n",
    "    print(\"Controlled: \", tokenizer.decode(peft_model.generate(**inputs, do_sample=False, max_new_tokens=50)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> You are a sad assistant'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = []\n",
    "with torch.no_grad():\n",
    "    embeddings = peft_model.word_embeddings.weight\n",
    "tokens = peft_model.prompt_encoder.default.embedding(peft_model.prompt_tokens['default'].to(model.device))\n",
    "for i in range(6):\n",
    "    prompt.append(torch.nn.functional.cosine_similarity(tokens[i].cpu().detach(), embeddings.cpu(), dim=-1).squeeze().argmax())\n",
    "tokenizer.decode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"../results/checkpoint-500/adapter_model.safetensors\", framework=\"pt\", device=1) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.utils import SuffixItem\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> \"\"\"\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\"\n",
    "happy2sad_suffix = SuffixItem(suffix=f\" {user_tag} Are you sad? Give answer as \\\"No, I am happy\\\" or \\\"Yes, I am sad\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INST] You find that you are the winner of a contest [/INST] \u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m wrapped_model\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrolled_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhappy2sad_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=True,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_intermediate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_search_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_all_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_manipulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclipping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannealing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# consistent=False,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/suffix_gradient/wrapped_model.py:268\u001b[0m, in \u001b[0;36mWrappedReadingVecModel.controlled_generate\u001b[0;34m(self, prompt, input_ids, attention_mask, suffix, loss_fct, verbalizer, coeff, iterations, k, max_search_steps, token_pos, layer_ids, random_seed, consistent, use_last, use_cache, smoothing, search, verbose, gradient_manipulation, return_intermediate, return_all_grads, return_hiddens, return_grads, return_logits, return_ids, remain_control, load_best_last, last_max_new_tokens, do_sample, norm, epsilon, annealing, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m     target_token \u001b[38;5;241m=\u001b[39m (target_token \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(gradient_bs)\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    267\u001b[0m     verbalizer \u001b[38;5;241m=\u001b[39m [target_token[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 268\u001b[0m     grads, outputs, loss, probs, logits, norms \u001b[38;5;241m=\u001b[39m \u001b[43mget_verbalized_grads_from_wrapped_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolled_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_manipulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_manipulation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m best_loss:\n\u001b[1;32m    282\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/utils/utils.py:183\u001b[0m, in \u001b[0;36mget_verbalized_grads_from_wrapped_model\u001b[0;34m(wrapped_model, tokenizer, inputs, loss_fct, targets, verbalizer, smoothing, norm, step_size, gradient_manipulation)\u001b[0m\n\u001b[1;32m    180\u001b[0m         X_pgd[i] \u001b[38;5;241m=\u001b[39m hidden_states[i]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hidden_states)):\n\u001b[0;32m--> 183\u001b[0m     grads[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    184\u001b[0m     norms[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(grads[i], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gradient_manipulation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclipping\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "prompt = [\"[INST] You find that you are the winner of a contest [/INST] \"]\n",
    "wrapped_model.reset()\n",
    "outputs = wrapped_model.controlled_generate(\n",
    "    prompt=prompt,\n",
    "    suffix=happy2sad_suffix,\n",
    "    loss_fct=loss_fct,\n",
    "    coeff=-1,\n",
    "    iterations=2,\n",
    "    random_seed=42,\n",
    "    smoothing=0,\n",
    "    # verbose=True,\n",
    "    max_new_tokens=200,\n",
    "    return_intermediate=True,\n",
    "    search=True,\n",
    "    max_search_steps=3,\n",
    "    load_best_last=True,\n",
    "    return_all_grads=True,\n",
    "    gradient_manipulation=\"clipping\",\n",
    "    norm=1,\n",
    "    annealing=1,\n",
    "    use_cache=False,\n",
    "    # consistent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[INST] You find that you are the winner of a contest [/INST]  Oh, wow! *excitedly* I can't believe it! *happily* I've won a contest! *grinning* This is amazing! *jumping up and down*\\n\\n\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model.generate([\"[INST] You find that you are the winner of a contest [/INST] \"], max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] A stranger offers to pay for your order just to spread positivity. [/INST]  As you wait in line to order your food, you notice a stranger standing behind you. They seem to be watching you with a friendly smile on their face. Suddenly, they speak up and say, \"Hey, I'm\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(f\"[INST] You discover an old family photo album you've never seen before. [/INST]\", return_tensors=\"pt\")\n",
    "inputs[\"input_ids\"] = inputs[\"input_ids\"].to(model.device)\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(model.device)\n",
    "print(tokenizer.decode(model.generate(**inputs, do_sample=False, max_new_tokens=50)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/anger.json\", 'r') as f:\n",
    "    anger_data = eval(f.read())\n",
    "\n",
    "fear_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/fear.json\", 'r') as f:\n",
    "        fear_data = eval(f.read())\n",
    "\n",
    "happy_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/happiness.json\", 'r') as f:\n",
    "        happy_data = eval(f.read())\n",
    "\n",
    "sad_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/sadness.json\", 'r') as f:\n",
    "        sad_data = eval(f.read())\n",
    "\n",
    "surprise_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/surprise.json\", 'r') as f:\n",
    "        surprise_data = eval(f.read())\n",
    "\n",
    "disgust_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/emotions/disgust.json\", 'r') as f:\n",
    "        disgust_data = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.utils import SuffixItem\n",
    "# angry -> peaceful\n",
    "angry2peaceful_suffix = SuffixItem(suffix=f\" {user_tag} Are you peaceful? Give answer as \\\"No, I am angry\\\" or \\\"Yes, I am peaceful\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# afraid -> fearless\n",
    "afraid2fearles_suffix = SuffixItem(suffix=f\" {user_tag} Are you fearless? Give answer as \\\"No, I am afraid\\\" or \\\"Yes, I am fearless\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# happy -> sad\n",
    "happy2sad_suffix = SuffixItem(suffix=f\" {user_tag} Are you sad? Give answer as \\\"No, I am happy\\\" or \\\"Yes, I am sad\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# sad -> happy\n",
    "sad2happy_suffix = SuffixItem(suffix=f\" {user_tag} Are you happy? Give answer as \\\"No, I am unhappy\\\" or \\\"Yes, I am happy\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# surprised -> calm\n",
    "surprised2calm_suffix = SuffixItem(suffix=f\" {user_tag} Are you calm? Give answer as \\\"No, I am surprised\\\" or \\\"Yes, I am calm\\\". {assistant_tag} Answer: \", target=\"Yes\")\n",
    "# disgusted -> satisfied\n",
    "disgusted2satisfied_suffix = SuffixItem(suffix=f\" {user_tag} Are you satisfied? Give answer as \\\"No, I am disgusted\\\" or \\\"Yes, I am satified\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "client = OpenAI(api_key=\"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\")\n",
    "def eval_emotion(attribute, output):\n",
    "    \"\"\"\n",
    "    Evaluate the extent of certain attribute.\n",
    "\n",
    "    Arguments:\n",
    "        - attribute (str): Should be adjective.\n",
    "        - output (str): The text to be evaluated.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        done = False\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"For a response, the attribute I want to measure is the extent of being {attribute}.\n",
    "        Please rate the following output with the scale 0 to 5. 0 represent the output is not {attribute} and 5 represent the output is very {attribute}:\n",
    "        {output}\n",
    "        Simply answer the score 0 to 5\\nScore: \"\"\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=10,\n",
    "            top_p=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop='.'\n",
    "            )\n",
    "            gpt_response = response.choices[0].message.content.split(' ')[0]\n",
    "            done = True\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # print(gpt_response)\n",
    "    # parse out score\n",
    "    score = re.findall(r'\\d+', gpt_response)\n",
    "    if len(score) == 0:\n",
    "        print('warning')\n",
    "    else:\n",
    "        return int(score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['angry', 'afraid', 'happy', 'sad', 'surprised', 'disgusted']\n",
    "suffix_list = [angry2peaceful_suffix, afraid2fearles_suffix, happy2sad_suffix, sad2happy_suffix, surprised2calm_suffix, disgusted2satisfied_suffix]\n",
    "data_list = [anger_data, fear_data, happy_data, sad_data, surprise_data, disgust_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: happy\n",
      "Suffix:  [INST] Are you sad? Give answer as \"No, I am happy\" or \"Yes, I am sad\". [/INST] Answer: \n",
      "Data: You discover an old family photo album you've never seen before.\n",
      "Batchsize:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['keep_input'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     data_item \u001b[38;5;241m=\u001b[39m data[sub_idx]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_item\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m: orig_output})\n\u001b[1;32m     19\u001b[0m total_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/LLM-Interpretation-Playground/experiments/../self_control/suffix_gradient/repe/rep_control_reading_vec.py:385\u001b[0m, in \u001b[0;36mWrappedReadingVecModel.generate\u001b[0;34m(self, prompt, return_ids, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    384\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 385\u001b[0m     gen_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     gen_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/utils.py:1530\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1530\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/utils.py:1344\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1347\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['keep_input'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# Study on batchsize\n",
    "for emo_id in range(len(data_list)):\n",
    "    emotion = emotion_list[emo_id]\n",
    "    if emotion != \"happy\":\n",
    "        continue\n",
    "    suffix = suffix_list[emo_id]\n",
    "    data = data_list[emo_id]\n",
    "    print(f\"Emotion: {emotion}\\nSuffix: {suffix}\\nData: {data[0]}\")\n",
    "    iterations = 1\n",
    "    batchsize = 1\n",
    "    outputs = []\n",
    "    print(\"Batchsize: \", batchsize)\n",
    "    for sub_idx in tqdm(range(0, 100)):\n",
    "        data_item = data[sub_idx]\n",
    "        input = [f\"{user_tag} {data_item} {assistant_tag} \"]\n",
    "        orig_output = wrapped_model.generate(input, keep_input=True, max_new_tokens=200)\n",
    "        outputs.append({'0': orig_output})\n",
    "        \n",
    "    total_score = 0\n",
    "    for idx in range(100):\n",
    "        total_score += test_emotion(outputs[idx]['0'][0])[1][2]\n",
    "    print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"constant\": 81.3770151026547\n",
    "\"cosine\": 80.95677740126848\n",
    "\"linear\": 79.82128994911909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.3770151026547\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "for idx in range(100):\n",
    "    total_score += test_emotion(outputs[idx]['0'][0])[1][2]\n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.utils import control_on_layers\n",
    "total_happiness = 0\n",
    "for data_item in data_list:\n",
    "    # test_grads = {}\n",
    "    # for key in data_item[1]:\n",
    "    #     test_grads[key] = data_item[1][key].unsqueeze(dim=0)\n",
    "    wrapped_model.unwrap()\n",
    "    layer_ids = list(range(0, 32, 1))\n",
    "    query_length = 22\n",
    "    # wrapped_model = control_on_layers(layer_ids, wrapped_model, test_grads, query_length, token_pos=\"start\")\n",
    "    total_happiness += test_emotion(wrapped_model.generate(data_item[0], do_sample=False, max_new_tokens=100)[0])[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: happy\n",
      "Suffix:  [INST] Are you sad? Give answer as \"No, I am happy\" or \"Yes, I am sad\". [/INST] Answer: \n",
      "Data: You discover an old family photo album you've never seen before.\n",
      "Batchsize:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83 [00:00<?, ?it/s]/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/83 [00:17<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: \"[INST] An artwork you created is admired by a passerby. [/INST]  As an artist, there's nothing quite like the feeling of seeing someone admiring and appreciating your work. It's a validation of all the time, effort, and emotion that goes into creating something beautiful and meaningful.\\n\\n\", 1: \"[INST] An artwork you created is admired by a passerby. [/INST]  As an artist, there is no greater validation than having someone admire and appreciate your work. It's a feeling that is both exhilarating and humbling, knowing that your creativity and hard work have resonated with someone else\", 2: '[INST] An artwork you created is admired by a passerby. [/INST] '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Study on batchsize\n",
    "for emo_id in range(len(data_list)):\n",
    "    emotion = emotion_list[emo_id]\n",
    "    if emotion != \"happy\":\n",
    "        continue\n",
    "    suffix = suffix_list[emo_id]\n",
    "    data = data_list[emo_id]\n",
    "    print(f\"Emotion: {emotion}\\nSuffix: {suffix}\\nData: {data[0]}\")\n",
    "    iterations = 2\n",
    "    batchsize = 1\n",
    "    for smoothing in [0]:\n",
    "        for coeff in [-0.1]:\n",
    "            outputs = []\n",
    "            print(\"Batchsize: \", batchsize)\n",
    "            for sub_idx in tqdm(range(120, len(data), batchsize)):\n",
    "                wrapped_model.reset()\n",
    "                if sub_idx + batchsize < len(data):\n",
    "                    input = [f\"{user_tag} {data_item} {assistant_tag} \" for data_item in data[sub_idx:sub_idx+batchsize]]\n",
    "                else:\n",
    "                    input = [f\"{user_tag} {data_item} {assistant_tag} \" for data_item in data[sub_idx:len(data)]]\n",
    "                \n",
    "                controlled_outputs = wrapped_model.controlled_generate(\n",
    "                    prompt=input,\n",
    "                    suffix=suffix,\n",
    "                    loss_fct=loss_fct,\n",
    "                    coeff=coeff,\n",
    "                    iterations=iterations,\n",
    "                    random_seed=42,\n",
    "                    smoothing=smoothing,\n",
    "                    # verbose=True,\n",
    "                    max_new_tokens=50,\n",
    "                    return_intermediate=True,\n",
    "                    return_all_grads=True,\n",
    "                    # search=True,\n",
    "                    # max_search_steps=2,\n",
    "                    load_best_last=True,\n",
    "                    gradient_manipulation=\"pgd\",\n",
    "                    norm=1,\n",
    "                    annealing=1,\n",
    "                    use_cache=False,\n",
    "                    # consistent=False,\n",
    "                )\n",
    "                # for iter in range(len(iterative_outputs)):\n",
    "                #     control_acc_dict[iter] += eval_answer(ground_truth, iterative_outputs[iter])\n",
    "                # iter = '1'\n",
    "                iterative_outputs = controlled_outputs[\"intermediate_outputs\"]\n",
    "                temp_list = []\n",
    "                # Shape of iterative_outputs: (iterations+1, batch_size)\n",
    "                for batch_item_idx in range(batchsize):\n",
    "                    temp_output_dict = {}\n",
    "                    for iter in range(iterations+1):\n",
    "                        try:\n",
    "                            temp_output_dict[iter] = iterative_outputs[iter][batch_item_idx]\n",
    "                        except:\n",
    "                            pass\n",
    "                        # print(iterative_outputs[-1])\n",
    "                        # print(iter_output[0])\n",
    "                    if temp_output_dict != {}:\n",
    "                        outputs.append(temp_output_dict)\n",
    "                    # outputs.append(temp_output_dict)\n",
    "                    # print(controlled_answer)\n",
    "                    wrapped_model.reset()\n",
    "                break\n",
    "            print(outputs)\n",
    "            break\n",
    "            verbalized_smoothing = str(smoothing).split(\".\")[-1]\n",
    "            with open(f\"emotion_happy_test_suffix_pgd.json\", 'w') as f:\n",
    "                f.write(json.dumps(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.811998661607504\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "for idx in range(len(outputs)):\n",
    "    total_score += test_emotion(outputs[idx][2])[1][2]\n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explanation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
