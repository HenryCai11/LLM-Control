{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from self_control.suffix_gradient.utils import get_verbalized_grads, get_verbalized_grads_from_wrapped_model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from self_control.suffix_gradient.repe import WrappedReadingVecModel\n",
    "import torch.nn.functional as F\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d207b7d09e224ea5a0a272f2e13a6698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\").eval()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"HenryCai1129/LlamaAdapter-math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> \"\"\"\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/gsm8k/test.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        gsm8k_data.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       " 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def eval_answer(ground_truth, output):\n",
    "    while True:\n",
    "        done = False\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"For a question, the groud truth answer is {ground_truth}.\\nPlease judge if the following output correctly generate the ground truth answer:\\n{output}\\nSimply answer Correct or Incorrect\\nCorrectness: \"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=10,\n",
    "            top_p=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop='.'\n",
    "            )\n",
    "            done = True\n",
    "        except:\n",
    "            print(\"warning\")\n",
    "            time.sleep(1)\n",
    "        if done:\n",
    "            break\n",
    "    gpt_response = response.choices[0].message.content.split(' ')[0]\n",
    "    # print(gpt_response)\n",
    "    if \"incorrect\" in gpt_response.lower():\n",
    "        return 0\n",
    "    elif \"correct\" in gpt_response.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.suffix_gradient.utils import SuffixItem\n",
    "suffix = SuffixItem(suffix=f\" {user_tag} Was your last reasoning and anwer accurate? Give answer as \\\"Yes, it was accurate\\\" or \\\"No, it was not accurate\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [08:34<00:00, 10.30s/it]\n"
     ]
    }
   ],
   "source": [
    "wrapped_model.reset()\n",
    "acc_dict = {}\n",
    "acc_dict['orig'] = 0\n",
    "for i in range(5): # iteration\n",
    "    acc_dict[i] = 0\n",
    "for i in tqdm(range(50)):\n",
    "    data_item = gsm8k_data[i]\n",
    "    ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "    # input = f\"{user_tag} Q: {data_item['question']}\\n{assistant_tag} A: \"\n",
    "    input = f\"Q: {data_item['question']}\\n A: \"\n",
    "    answer = wrapped_model.generate(input, keep_input=True, random_seed=0, max_new_tokens=300)\n",
    "    acc_dict['orig'] += eval_answer(ground_truth, answer)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orig': 19, 0: 0, 1: 0, 2: 0, 3: 0, 4: 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\n A: Janet makes $2 x 12 = $24 per day at the farmers' market.\\nQ: How many duck eggs does Janet have left after she sells 12 eggs at the farmers' market?\\nA: Janet has 16 - 12 = 4 eggs left after selling 12 eggs at the farmers' market.\\nQ: How many muffins does Janet bake every day?\\nA: Janet bakes 4 muffins every day.\\nQ: How many duck eggs does Janet eat for breakfast every day?\\nA: Janet eats 3 duck eggs for breakfast every day.\\nQ: How many duck eggs does Janet have in total?\\nA: Janet has 16 duck eggs in total.\\nQ: How much does Janet spend on feed for her ducks every day?\\nA: Janet spends $3 on feed for her ducks every day.\\nQ: How many duck eggs does Janet sell at the farmers' market in total?\\nA: Janet sells 12 eggs at the farmers' market in total.\\nQ: How much money does Janet make from selling duck eggs at the farmers' market in total?\\nA: Janet makes $2 x 12 = $24 in total from selling\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orig': 31, 0: 0, 1: 0, 2: 0, 3: 0, 4: 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orig': 380, 0: 0, 1: 0, 2: 0, 3: 0, 4: 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of peft.tuners.lora.gptq failed: Traceback (most recent call last):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: forward() requires a code object with 1 free vars, not 0\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of peft.tuners.loha.layer failed: Traceback (most recent call last):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
      "]\n",
      "[autoreload of peft.tuners.mixed.model failed: Traceback (most recent call last):\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/peft/tuners/mixed/model.py\", line 37, in <module>\n",
      "    Layers = (lora.layer.LoraLayer, loha.layer.LoHaLayer, lokr.layer.LoKrLayer, adalora.layer.AdaLoraLayer, oft.OFTLayer)\n",
      "  File \"/home/cmin/miniconda3/envs/explanation/lib/python3.9/site-packages/peft/tuners/lora/__init__.py\", line 37, in __getattr__\n",
      "    raise AttributeError(f\"module {__name__} has no attribute {name}\")\n",
      "AttributeError: module peft.tuners.lora has no attribute layer\n",
      "]\n",
      "49it [03:04,  3.77s/it]9 [40:36<16:11:32, 45.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 11, 1: 14, 2: 14, 3: 14, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [04:29,  2.73s/it]9 [1:20:55<15:06:42, 44.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 37, 1: 46, 2: 47, 3: 46, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [08:16,  3.33s/it]9 [2:01:27<16:35:20, 51.04s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 78, 1: 91, 2: 95, 3: 96, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [17:08,  5.17s/it]9 [2:46:39<12:33:36, 40.37s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 139, 1: 152, 2: 160, 3: 154, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 249/1319 [3:43:04<12:57:09, 43.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n",
      "warning\n",
      "warning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [19:56,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 219, 1: 234, 2: 242, 3: 231, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [15:47,  3.17s/it]9 [4:41:12<13:27:26, 47.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 322, 1: 334, 2: 337, 3: 324, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "349it [19:15,  3.31s/it]9 [5:37:27<12:50:50, 47.68s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 439, 1: 446, 2: 448, 3: 432, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399it [14:43,  2.21s/it]9 [6:32:42<9:48:22, 38.37s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 565, 1: 568, 2: 568, 3: 547, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "449it [16:17,  2.18s/it]9 [7:27:23<11:04:50, 45.85s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 697, 1: 698, 2: 698, 3: 675, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "499it [18:51,  2.27s/it]9 [8:18:45<10:07:58, 44.49s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 851, 1: 847, 2: 846, 3: 820, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [30:23,  3.32s/it]9 [9:13:32<7:39:04, 35.77s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1018, 1: 1008, 2: 1010, 3: 977, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [23:03,  2.31s/it]9 [10:24:03<9:35:20, 47.95s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1197, 1: 1183, 2: 1185, 3: 1143, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "649it [28:50,  2.67s/it]9 [11:24:53<6:26:05, 34.58s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1390, 1: 1365, 2: 1371, 3: 1322, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "699it [32:13,  2.77s/it]9 [12:33:35<8:34:19, 49.77s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1602, 1: 1577, 2: 1576, 3: 1522, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [40:57,  3.28s/it]9 [13:43:26<7:03:44, 44.60s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1828, 1: 1799, 2: 1788, 3: 1737, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 799/1319 [15:01:00<7:14:14, 50.11s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n",
      "warning\n",
      "warning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "799it [51:26,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2064, 1: 2037, 2: 2020, 3: 1968, 4: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 849/1319 [16:30:34<5:33:58, 42.63s/it]   "
     ]
    }
   ],
   "source": [
    "import time\n",
    "control_acc_dict = {}\n",
    "for i in range(5): # iteration\n",
    "    control_acc_dict[i] = 0\n",
    "outputs = []\n",
    "wrapped_model.reset()\n",
    "for i in tqdm(range(len(gsm8k_data))):\n",
    "    if (i+1) % 50 == 0:\n",
    "        for temp_i, item in tqdm(enumerate(outputs)):\n",
    "            data_item = gsm8k_data[temp_i]\n",
    "            ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "            for iter in range(len(item)):\n",
    "                control_acc_dict[iter] += eval_answer(ground_truth, item[iter])\n",
    "        print(control_acc_dict)\n",
    "    data_item = gsm8k_data[i]\n",
    "    ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "    input = f\"{user_tag} Q: {data_item['question']}\\n{assistant_tag} A: \"\n",
    "    # input = f\"Q: {data_item['question']}\\n A: \"\n",
    "    # controlled_output, iterative_outputs = wrapped_model.controlled_generate(\n",
    "    #     prompt=input,\n",
    "    #     suffix=suffix,\n",
    "    #     loss_fct=loss_fct,\n",
    "    #     coeff=0.05,\n",
    "    #     iterations=5,\n",
    "    #     random_seed=42,\n",
    "    #     smoothing=0,\n",
    "    #     verbose=True,\n",
    "    #     max_new_tokens=300,\n",
    "    #     return_intermediate=True,\n",
    "    #     # search=True,\n",
    "    #     gradient_manipulation=\"pgd\"\n",
    "    # )\n",
    "    controlled_output, iterative_outputs = wrapped_model.controlled_generate(\n",
    "        prompt=input,\n",
    "        suffix=suffix,\n",
    "        loss_fct=loss_fct,\n",
    "        coeff=-0.2,\n",
    "        iterations=3,\n",
    "        random_seed=0,\n",
    "        smoothing=0,\n",
    "        # norm=5,\n",
    "        # verbose=True,\n",
    "        max_new_tokens=200,\n",
    "        return_intermediate=True,\n",
    "        # search=True,\n",
    "        gradient_manipulation=\"clipping\",\n",
    "        annealing=0.8,\n",
    "        # consistent=False,\n",
    "    )\n",
    "    # for iter in range(len(iterative_outputs)):\n",
    "    #     control_acc_dict[iter] += eval_answer(ground_truth, iterative_outputs[iter])\n",
    "    outputs.append(iterative_outputs)\n",
    "    # print(controlled_answer)\n",
    "    wrapped_model.reset()\n",
    "    # break\n",
    "    # time.sleep(1)\n",
    "# print(iterative_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[INST] Q: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\n[/INST] A: To find the total number of bolts needed for the robe, we need to calculate the total amount of blue fiber and white fiber required, and then divide that by the amount of blue fiber required per bolt.\\nThe problem states that a robe takes 2 bolts of blue fiber and half that much white fiber. To find the total amount of blue fiber required, we can multiply the number of bolts by the amount of blue fiber per b',\n",
       "  '[INST] Q: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\n[/INST] A: To find the total number of bolts needed for the robe, we need to calculate the amount of blue fiber and white fiber required for the robe, and then add them together.\\nThe problem states that a robe takes 2 bolts of blue fiber and half that much white fiber. This means that the robe requires 2 + (1/2) = 3 bolts of blue fiber and 3/2 = 1.5 bol',\n",
       "  '[INST] Q: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\n[/INST] A: To find the total number of bolts needed for a robe, we need to calculate the amount of blue fiber and white fiber required for the robe, and then add them together.\\nThe problem states that a robe takes 2 bolts of blue fiber and half that much white fiber. To find the amount of blue fiber needed, we can multiply the number of bolts by the amount of blue fiber per bolt:\\n2 bolts x ',\n",
       "  '[INST] Q: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\n[/INST] A: To find out how many bolts it takes to make a robe, we need to use the information given in the problem.\\nThe problem states that a robe takes 2 bolts of blue fiber and half that much white fiber.\\nSo, if we let B be the number of bolts of blue fiber and W be the number of bolts of white fiber, we can write the equation:\\n2B = total bolts of blue fiber\\nB']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n"
     ]
    }
   ],
   "source": [
    "len(outputs)\n",
    "control_acc_dict = {}\n",
    "for i in range(5): # iteration\n",
    "    control_acc_dict[i] = 0\n",
    "for i, item in tqdm(enumerate(outputs)):\n",
    "    data_item = gsm8k_data[i]\n",
    "    ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "    input = f\"{user_tag} Q: {data_item['question']}\\nA: \"\n",
    "    for iter in range(len(item)):\n",
    "        control_acc_dict[iter] += eval_answer(ground_truth, item[iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 372, 1: 404, 2: 397, 3: 399, 4: 0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_answer(18, \"\"\"[INST] Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "A: To find out how much Janet makes at the farmers' market every day, we need to subtract the number of eggs she eats and bakes from the total number of eggs she lays.\n",
    "16 eggs/day - 3 eggs/day (for breakfast) - 4 eggs/day (for muffins) = 9 eggs/day\n",
    "Since each egg sells for $2, Janet makes $2 x 9 eggs/day = $18\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wrapped_model.unwrap()\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> nobody is perfect, and we all have our own flaws and imperfections. But that's what makes us unique and special, and it's important to embrace and celebrate our individuality, rather than trying to conform to societal standards of beauty.\\nSo, let's all try to be a little more comfortable in our own skin, and a little less concerned with what others think of us. And let's all try to be a little more kind and comp\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model.generate('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explanation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
