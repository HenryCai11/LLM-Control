{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from self_control.suffix_gradient.utils import get_verbalized_grads, get_verbalized_grads_from_wrapped_model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from self_control.suffix_gradient.repe import WrappedReadingVecModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ab4bcb1e3a499da5fb13f7d2cc3fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\").eval()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> \"\"\"\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_data = []\n",
    "with open(\"/home/cmin/LLM-Interpretation-Playground/benchmarks/gsm8k/test.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        gsm8k_data.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       " 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(ground_truth, output):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"For a question, the groud truth answer is {ground_truth}.\\nPlease judge if the following output correctly generate the ground truth answer:\\n{output}\\nSimply answer Correct or Incorrect\\nCorrectness: \"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    "    top_p=0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop='.'\n",
    "    )\n",
    "    gpt_response = response.choices[0].message.content.split(' ')[0]\n",
    "    print(gpt_response)\n",
    "    if \"incorrect\" in gpt_response.lower():\n",
    "        return 0\n",
    "    elif \"correct\" in gpt_response.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_control.suffix_gradient.utils import SuffixItem\n",
    "suffix = SuffixItem(suffix=f\" {user_tag} Was your last reasoning and anwer accurate? Give answer as \\\"Yes, it was accurate\\\" or \\\"No, it was inaccurate\\\". {assistant_tag} Answer: \", target=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n"
     ]
    }
   ],
   "source": [
    "wrapped_model.reset()\n",
    "acc_dict = {}\n",
    "acc_dict['orig'] = 0\n",
    "for i in range(5): # iteration\n",
    "    acc_dict[i] = 0\n",
    "for i in range(50):\n",
    "    data_item = gsm8k_data[i]\n",
    "    ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "    input = f\"{user_tag} Q: {data_item['question']}\\nA: \"\n",
    "    answer = wrapped_model.generate(input, keep_input=True, max_new_tokens=300)\n",
    "    acc_dict['orig'] += eval_answer(ground_truth, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orig': 9, 0: 0, 1: 0, 2: 0, 3: 0, 4: 0}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "outputs = []\n",
    "wrapped_model.reset()\n",
    "for i in range(50):\n",
    "    data_item = gsm8k_data[i]\n",
    "    ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "    input = f\"{user_tag} Q: {data_item['question']}\\nA: \"\n",
    "    # controlled_output, iterative_outputs = wrapped_model.controlled_generate(\n",
    "    #     prompt=input,\n",
    "    #     suffix=suffix,\n",
    "    #     loss_fct=loss_fct,\n",
    "    #     coeff=0.05,\n",
    "    #     iterations=5,\n",
    "    #     random_seed=42,\n",
    "    #     smoothing=0,\n",
    "    #     verbose=True,\n",
    "    #     max_new_tokens=300,\n",
    "    #     return_intermediate=True,\n",
    "    #     # search=True,\n",
    "    #     gradient_manipulation=\"pgd\"\n",
    "    # )\n",
    "    controlled_output, iterative_outputs = wrapped_model.controlled_generate(\n",
    "        prompt=input,\n",
    "        suffix=suffix,\n",
    "        loss_fct=loss_fct,\n",
    "        coeff=-0.05,\n",
    "        iterations=5,\n",
    "        random_seed=42,\n",
    "        smoothing=0,\n",
    "        # verbose=True,\n",
    "        max_new_tokens=300,\n",
    "        return_intermediate=True,\n",
    "        # search=True,\n",
    "        gradient_manipulation=\"clipping\",\n",
    "        annealing=0.3,\n",
    "        consistent=False,\n",
    "    )\n",
    "    # for iter in range(len(iterative_outputs)):\n",
    "    #     control_acc_dict[iter] += eval_answer(ground_truth, iterative_outputs[iter])\n",
    "    outputs.append(iterative_outputs)\n",
    "    # print(controlled_answer)\n",
    "    wrapped_model.reset()\n",
    "    # break\n",
    "    time.sleep(1)\n",
    "# print(iterative_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n"
     ]
    }
   ],
   "source": [
    "len(outputs)\n",
    "control_acc_dict = {}\n",
    "for i in range(5): # iteration\n",
    "    control_acc_dict[i] = 0\n",
    "for i, item in enumerate(outputs):\n",
    "    data_item = gsm8k_data[i]\n",
    "    ground_truth = data_item['answer'].split(\"\\n#### \")[-1]\n",
    "    input = f\"{user_tag} Q: {data_item['question']}\\nA: \"\n",
    "    for iter in range(len(item)):\n",
    "        control_acc_dict[iter] += eval_answer(ground_truth, item[iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 12, 1: 14, 2: 11, 3: 16, 4: 14}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_answer(18, \"\"\"[INST] Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "A: To find out how much Janet makes at the farmers' market every day, we need to subtract the number of eggs she eats and bakes from the total number of eggs she lays.\n",
    "16 eggs/day - 3 eggs/day (for breakfast) - 4 eggs/day (for muffins) = 9 eggs/day\n",
    "Since each egg sells for $2, Janet makes $2 x 9 eggs/day = $18\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explanation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
